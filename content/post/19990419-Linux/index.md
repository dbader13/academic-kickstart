---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Plugged In: Linux Showing Up In Supercomputers"
subtitle: ""
summary: ""
authors: []
tags: []
categories: []
date: 1999-04-19T16:06:24-04:00
lastmod: 1999-04-19T16:06:24-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

*By Therese Poletti*

Linux -- the renegade operating system that is among the hottest topics in
Silicon Valley -- is also making its way into the most serious bastion of computing, the supercomputing world.

Linux, developed by Finnish programmer Linus Torvalds in 1991, is given away over the Internet and managed by a far-flung
group of programmers, part of what is known as the open source movement. Linux has been catching on among some
corporations and Internet service providers as a reliable system to run Web servers or e-mail servers.

Several high-performance computing centers, universities and government laboratories are also looking at Linux, inspired by
its low cost, its development model of sharing software code and its closeness to Unix, the operating system typically
preferred by engineers and serious computer designers.

‘‘Some of the supercomputing research community would like to start moving to Linux,’’ said Irving Wladawsky-Berger, the
general manager of IBM’s Internet division and former head of the computer giant’s supercomputing business. ‘‘In the
high-end supercomputing world, everyone is a small community and that model (of open source software) is very appealing.’’

Supercomputing represents a slow-growing $2.2 billion segment of the computer industry, where massive systems are now
achieving speeds in excess of one teraflop: one trillion operations per second. They are used for scientific ‘‘grand challenges,’’
such as weather forecasting, nuclear simulations, molecular modeling, and many other number-crunching intensive
applications where machines can work on a problem for a week.

While Linux is not yet running any of the ultrafast, teraflop-level machines, it is now being used by a few supercomputing
centers in so-called clusters or superclusters.

Scalable clustered systems are more powerful than a desktop workstation, but not quite as hefty as the multimillion-dollar
supercomputers, the fastest computers in the world. Scalable means that they can add more processors, to improve
performance or to add additional users.

In 1994, the National Aeronautics and Space Administration (NASA) pioneered the use of Linux for building extremely
cheap clusters with a project called the Beowulf project, building very low-cost clusters with off-the-shelf computer parts. But
these sprawling systems took up a lot of floor space and there was no computer maker to support the patched-together
systems.

So as funding is obtained, some of the high-performance computing centers are now buying cluster computers running Intel
Pentium II processors -- the brains of a PC -- and the Linux operating system.

Just last week, the Albuquerque High Performance Computing Center, located on the University of New Mexico campus,
turned on a workstation supercluster system it calls Roadrunner, which basically consists of stacks of personal computer
technology running multiple Intel Corp. (Nasdaq:INTC - news) Pentium II processors and Linux.

Albuquerque bought its $400,000 system from a small, privately held company called Alta Technology Corp., based in
Sandy, Utah, which develops clustered computer systems starting at $15,000, with either Intel processors or Digital’s Alpha
processor.

Albuquerque’s Roadrunner has 128 Intel Pentium II processors, running at speeds of 450 megahertz, similar to the massively
parallel supercomputing systems which gang together multiple processors and distribute the work among the chips.
‘‘We are not trying to reinvent the supercomputer,’’ said **David Bader**, an assistant professor of computer engineering at the
University of New Mexico. ‘‘We hope to get maybe half the performance at 10 percent of the price.’’ Albuquerque will be
looking at environmental modeling, such as computing the climate in the Rio Grande corridor, and simulations on nuclear
stockpiles under certain conditions and of accidents involving trucks carrying nuclear waste.

And with Linux, Albuquerque’s engineers will be able to share their work with other colleagues at other supercomputing
centers, because Linux runs on Compaq Computer Corp. (NYSE:CPQ - news)’s Digital Alpha processor, Sun Microsystems
Inc. (Nasdaq:SUNW - news)’s Sparc technology, IBM’s PowerPC processor architecture and others.

This will be especially useful for Roadrunner, which is the latest system to be connected to what is called the National
Technology Grid, an emerging network that will link a broad range of supercomputers from Boston to Maui, so that scientists
around the United States, far from the centers, can have access to vast computing power without having to leave their own
desks.

‘‘Linux has already been ported to machines made by most of the major vendors, unifying the marketplace instead of
fragmenting it,’’ said Pete Beckman, a senior computer scientist at the Advanced Computing Laboratory at the Los Alamos
National Laboratory in Los Alamos, N.M. ‘‘Laboratories from around the world can collaborate more easily, sharing and
testing extensions and improvements made to or for Linux ... without being hampered by non-disclosure agreements and
licensing restrictions (for vendor-controlled software).’’

Los Alamos is experimenting on several applications with its own Linux cluster system from Alta Tech, which it calls the
Little Blue Penguin, installed about eight months ago.

Some applications at Los Alamos include a computational accelerator, which models a linear accelerator with 200 million
particles, and an ocean modeling code that is part of a global climate modeling project.

Linux is competitive in many areas of high-performance computing, but there are several areas where it falls short, with
missing components. For example, the Linux kernel -- the core of the operating system -- has not been optimized to run on
large shared memory machines with eight to 128 processors.

Beckman said, however, that there are either commercial or open source software development projects addressing Linux’s
shortfalls in high-performance computing.

‘‘What we are seeing here across all the national labs is really an unprecedented cooperation with Linux clustering,’’ said
Remy Evard, manager of advanced computing at Argonne National Laboratory, operated by the University of Chicago for the
U.S. Department of Energy.
