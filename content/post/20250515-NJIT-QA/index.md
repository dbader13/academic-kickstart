---
# Documentation: https://hugoblox.com/docs/managing-content/

title: "Q&A with David Bader, NJIT Magazine, Spring 2025"
subtitle: ""
summary: ""
authors: []
tags: []
categories: []
date: 2025-05-15T14:40:17-04:00
lastmod: 2025-05-15T14:40:17-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

## Q&A with **David Bader**, Distinguished Professor, Data Science ##

## AI and Society ##

A conversation with Evan Koblentz and David Bader

Q: **A lot of people who are information workers are afraid that AI will
make their careers obsolete. Technological progress can’t be stopped,
so how should people adapt?**

A: In the face of technological progress, particularly with the rapid
advancement of AI, it’s understandable that information workers may
feel apprehensive about the future of their careers. However, rather
than viewing AI as a harbinger of obsolescence, it’s crucial to see it
as a catalyst for evolution and innovation in our work practices. The
key to adapting is in embracing these technologies, learning to work
alongside them, and leveraging their capabilities to enhance our own
skill sets and productivity. The first step in this adaptation process
is to cultivate a mindset of lifelong learning. As AI and other
technologies continue to evolve, so too must our skills and
knowledge. This means staying informed about new technologies, seeking
out educational opportunities, and being open to acquiring new
competencies that complement the capabilities of AI.

For instance, developing skills in data literacy, AI ethics and
understanding the principles of machine learning can make workers more
versatile and valuable in an AI-integrated workplace. Additionally,
it’s important to focus on the uniquely human skills that AI cannot
replicate, such as creativity, emotional intelligence and critical
thinking. By honing these abilities, workers can ensure they remain
irreplaceable components of the workforce, capable of tasks that
require a human touch, from complex decision-making to empathetic
interactions with customers or clients.

Q: **Other than creative prompt-making, what should non-programmers
learn now about AI?**

A: For non-programmers looking to delve deeper into AI, understanding
the ethical implications and societal impacts of AI is paramount. It’s
important to be aware of how AI decisions are made, the potential
biases in AI systems, and the ethical considerations of AI
use. Additionally, developing data literacy is crucial, as it enables
individuals to evaluate AI outputs and understand the importance of
data quality and biases. A basic grasp of AI and machine learning
concepts, even without programming skills, can demystify AI
technologies and reveal their potential applications. Staying informed
about AI advancements across various sectors can also inspire
innovative ideas and foster interdisciplinary collaborations. By
focusing on these areas, non-programmers can contribute meaningfully
to the AI conversation and its future direction.

Q: **There’s a popular sci-fi plot where the computers get so smart that
people lose control. The new class of user-friendly AI is certainly
making people excited but also nervous. Should we be afraid?**

A: The emergence of user-friendly AI technologies has indeed brought
this conversation into the mainstream, highlighting the balance we
must strike between harnessing the benefits of AI and addressing valid
concerns about its implications. It’s critical to recognize that the
AI technologies we’re creating today are built with numerous
safeguards, are subject to ethical guidelines, and operate within
evolving regulatory environments.  These measures are designed to
ensure AI systems augment human abilities and decision-making, rather
than supplanting or undermining human control.

While it’s natural to harbor concerns about the rapid progression of
AI, allowing fear to dominate the discourse would be a disservice to
the potential benefits these technologies can offer. Instead, this
moment calls for proactive engagement with AI, an investment in
understanding its inner workings, limitations and the ethical dilemmas
it presents. By advocating for responsible AI development, emphasizing
education and promoting transparency, we can foster an environment
where AI serves as a tool for societal advancement. This approach
ensures that we remain at the helm of AI’s trajectory, steering it
towards outcomes that uplift humanity rather than scenarios that fuel
dystopian fears.

Q: **You and your peers at the Institute for Data Science (IDS) are
known for researching the building blocks and tools that help make AI
infrastructure possible.  Specifically, what would you say are IDS’
most important contributions so far?**

A: The Institute for Data Science at NJIT has made groundbreaking
contributions to graph analytics and high-performance computing
through the development of Arachne, a sophisticated and open-source
framework for processing massivescale graphs. At its foundation,
Arachne implements a hybrid edge list and adjacency structure that
revolutionizes how large-scale graphs are processed.  This
architectural innovation represents a significant leap forward in
handling the complexities of large-scale network analysis.

Beyond its technical architecture Arachne has demonstrated remarkable
versatility in real-world applications. In cybersecurity, it enables
rapid detection of emerging threat patterns through its pattern
matching capabilities. The framework’s ability to track community
structure and identify anomalies has proven valuable for social
network analysis, while its high-performance processing has enhanced
financial fraud detection systems. These advances in graph processing
highlight IDS’ broader contributions to high-performance computing.

The significance of these contributions extends beyond their immediate
applications. As artificial intelligence systems increasingly rely on
graph-based representations for processing complex relationships, the
optimizations and algorithms developed at IDS, particularly through
Arachne, have become fundamental to making these systems more
practical and scalable. Their work continues to bridge the gap between
theoretical computer science and practical applications, enabling the
next generation of AI infrastructure through innovative approaches to
graph processing and high-performance computing.

Q: **Some of your own research focuses on democratizing supercomputing
power.  Can that help lead to another approach for AI equal access?**

A: The concept of democratizing supercomputing offers intriguing
possibilities for expanding AI access.  When we consider how
democratized supercomputing could influence AI development, several
key pathways emerge. Fundamentally, the process of making
high-performance computing more accessible to diverse researchers and
institutions, rather than concentrating it among elite organizations,
could reshape how AI capabilities develop and spread.

As supercomputing becomes more widely available, smaller organizations
and independent researchers gain the ability to train and run AI
models without massive capital investments in dedicated hardware. This
democratization creates opportunities for innovation from previously
excluded participants in the AI development landscape.

However, the path to democratized AI through supercomputing faces
several significant challenges. Computing power, while crucial,
represents just one element in the complex ecosystem of AI
development. Equal consideration must be given to data access,
technical expertise and algorithmic innovation.  The environmental
impact of distributed supercomputing systems requires careful
assessment, particularly regarding energy consumption. Additionally,
any distributed computing approach to AI development must address
robust security and privacy protections.

This intersection of democratized supercomputing and AI access
highlights broader questions about how we can make artificial
intelligence technology more equitable and accessible while
maintaining necessary safeguards and standards.

Q: **How does supercomputer democratization impact the overall work of
the Institute for Data Science?**

A: The rise of user-friendly artificial intelligence systems like
ChatGPT marks a pivotal moment in our pursuit to democratize data
science and supercomputing. For my work, this evolution serves as both
a tool and a testament to the power of making complex computational
capabilities accessible to a broader audience. It enriches the palette
of methodologies and technologies at our disposal, enabling us to
tackle more ambitious projects with greater efficiency and
creativity. By integrating these AI systems into our research and
educational programs, we’re not just enhancing our ability to process
and analyze data, we’re also empowering students and researchers with
the means to innovate and explore new horizons in data science without
being hindered by the technical complexities that once acted as
barriers. For the Institute for Data Science, the impact of such AI
systems is transformative. They serve as a bridge between advanced
computational technologies and a diverse range of disciplinary
domains, facilitating interdisciplinary research and collaboration.

Q: **A new model from China, called DeepSeek, seems to be as good as
Western models but has far lower costs and technology
requirements. How did they do it, and what can Western companies learn
from this?**

A: DeepSeek’s emergence represents a significant challenge to
established thinking about AI development. While Western companies
have typically pursued AI advancement through massive computational
resources and extensive funding, DeepSeek has demonstrated that
remarkable results can be achieved through more efficient methods and
careful engineering.  The company’s approach centers on targeted
reinforcement learning focused specifically on reasoning tasks, rather
than the broader supervised learning methods common in Western models.
They’ve developed an innovative rulebased reward system that actually
outperforms traditional neural reward models, while using
significantly fewer resources. Perhaps most impressively, they’ve
managed to compress advanced capabilities into relatively small
models, achieving with 1.5 billion parameters what others do with far
larger models.

The financial implications are striking.  DeepSeek developed their R1
model for less than $6 million, a fraction of the hundreds of millions
typically spent by Western competitors. They’ve translated this cost
efficiency into their pricing model, offering services at $0.55 for
input and $2.19 for output per million tokens, substantially
undercutting market rates while maintaining comparable quality.

This success suggests that the future of AI development might lie more
in clever engineering and efficient methodology than in raw
computational power. It challenges the assumption that advanced AI
development requires massive resources and suggests that innovative
approaches to training and model architecture might be more important
than sheer scale. The success of DeepSeek also suggests that
competitive advantage in AI might come from unexpected directions, and
that the barriers to entry for significant AI advancement might be
lower than previously thought. It’s a reminder that technological
breakthroughs often come not from doing things bigger, but from doing
them smarter.

*- Evan Koblentz*

https://magazine.njit.edu/
