---
# Documentation: https://sourcethemes.com/academic/docs/managing-content/

title: "Gigantic clusters: Where are they and what are they doing?"
subtitle: ""
summary: ""
authors: []
tags: []
categories: []
date: 2000-04-01T22:26:39-04:00
lastmod: 2000-04-01T22:26:39-04:00
featured: false
draft: false

# Featured image
# To use, add an image named `featured.jpg/png` to your page's folder.
# Focal points: Smart, Center, TopLeft, Top, TopRight, Left, Right, BottomLeft, Bottom, BottomRight.
image:
  caption: ""
  focal_point: ""
  preview_only: false

# Projects (optional).
#   Associate this post with one or more of your projects.
#   Simply enter your project's folder or file name without extension.
#   E.g. `projects = ["internal-project"]` references `content/project/deep-learning/index.md`.
#   Otherwise, set `projects = []`.
projects: []
---

### COLLOSAL CLUSTERS AT ALLIANCE CENTERS ###

The Albuquerque High Performance Computing Center at
the University of New Mexico has long been a proponent of
colossal clusters. The AHPCC and the National Computational
Science Alliance (the Alliance), comprising more than 50
academic, government, and industry research partners from
across the US, have formed a partnership that the National
Science Foundation funds. The Alliance, which wants to provide
an advanced computational infrastructure, is running a
128-processor Linux SuperCluster with Myrinet (Roadrunner)
from Alta Technologies using dual Intel 450-MHz nodes,
each with 512 Mbytes of RAM. The AHPCC is acquiring a
512-processor Linux SuperCluster known as Los Lobos,
reports **David Bader** of the University of New Mexico. The
Alliance intends to make Los Lobos the largest open-production
Linux supercluster geared to the research community.

Los Lobos uses dual Intel 733-MHz IA-32 processor
Netfinity 4500R nodes; 1 Gbyte of memory per node; 1 Tbyte
of SSA RAID; and 2 Tbytes of tertiary storage (tape robot) to
deliver a peak theoretical performance of 375 Gflops. The
high-performance interconnect network between the cluster
nodes is Myricom’s Myrinet, providing speeds exceeding 1
Gbits per second, which is comparable
to the fastest interconnects in today’s
traditional supercomputers. The system
maximizes the computing power per
square foot on an Intel-based platform.
This thin server is designed to deliver
the highest computing power per square
foot on Intel-based platforms and is one
of the industry’s most complete rackoptimized
product lines for Linux, Windows,
and Novell servers, according to
IBM. Patricia Kovatch, High Performance
Computing Systems Group
manager, thinks Linux clusters are compelling
for several reasons, especially for
the cost-performance ratio. Furthermore, the cost is much
less than buying a traditional supercomputer, and the performance
rivals one. Another benefit for applications folks is the
ease of porting their applications to Linux from other Unix
environments. The prospects for the near future look even
more momentous: “Multiple multiple-terascale Linux-based
superclusters will be built in the next year with a 10-terascale
or better Linux supercluster highly likely in about a year,”
says Frank Gilfeather, Executive Director of High Performance
Computing.

Another Alliance partner, the National Center for Supercomputing
Applications at the University of Illinois at Urbana-
Champaign, is running a 128-processor dual-Pentium-III
Xeon, 550-MHz, 1-Gbyte RAM, and Myrinet network technology
on an NT operating system.
