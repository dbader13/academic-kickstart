@inproceedings{2024-VDB,
 author = {Fernando P. {Vera Buschmann} and Zhihui Du and David A Bader},
 booktitle = {The 28th Annual IEEE High Performance Extreme Computing Conference (HPEC), Virtual, September 23-27, 2024},
 groups = {Edited},
 title = {Enhanced Knowledge Graph Attention Networks for Efficient Graph Learning},
 year = {2024},
 url       = {},
 doi={10.1109/HPEC62836.2024.10938526},
 abstract = {This paper introduces an innovative design for Enhanced Knowledge Graph Attention Networks (EKGAT), focusing on improving representation learning for graph-structured data. By integrating TransformerConv layers, the proposed EKGAT model excels in capturing complex node relationships compared to traditional KGAT models. Additionally, our EKGAT model integrates disentanglement learning techniques to segment entity representations into independent components, thereby capturing various semantic aspects more effectively. Comprehensive experiments on the Cora, PubMed, and Amazon datasets reveal substantial improvements in node classification accuracy and convergence speed. The incorporation of TransformerConv layers significantly accelerates the convergence of the training loss function while either maintaining or enhancing accuracy, which is particularly advantageous for large-scale, real-time applications. Results from t-SNE and PCA analyses vividly illustrate the superior embedding separability achieved by our model, underscoring its enhanced representation capabilities. These findings highlight the potential of EKGAT to advance graph analytics and network science, providing robust, scalable solutions for a wide range of applications, from recommendation systems and social network analysis to biomedical data interpretation and real-time big data processing.}
}
